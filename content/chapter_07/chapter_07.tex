%!TEX root = ../../thesis_master.tex

%%%%%%%%%%
\chapter{Conclusions and Future Work}
\label{chap:conclusions-future-work}
%%%%%%%%%%

\section{Conclusions}

In this thesis, the system development of a Image Based Visual Servoing controller to control the translational and yaw degrees of freedom of an aerial robot has been described through all its phases. 

The process started with a review of the theoretical background of a IBVS controller and an analysis of similar systems. Since the final goal of the system is to work in aerial manipulation, the use of visual servoing in aerial manipulators was also covered in this part.

In the requirements specification and system analysis, the project goals and objectives where translated into fix requirements for a later implementation. After at this stage, the desired system was defined and the next step would be design all the system components so that such requirements were satisfied.

Within the system design the concrete strategies and algorithms to achieve the imposed requirements where analyzed an developed. The target was chosen to be an AprilTag, whose corners can be detected reliably and form an square. Based on this information, the centroid of the target, its area and orientation where selected as visual features. Under the assumption of smooth maneuvers, it can be said that the image plane is always parallel to the target. Thanks to this property, the relationship between the image features and the camera velocities is decoupled. Making possible to control each of the velocities with one of the selected features.

The difference between the desired and current features is used as error for a PID controller. A PID controller computes the velocities that make the error zero and ensures an adequate system performance. The desired camera velocities are transformed to the body frame of the aerial robot and commanded to the low-level controllers. When the robot moves, the new image provides a new set of features, closing the loop.

During the implementation phase, the designed system was materialized through its implementation as a ROS component so it can interact with the rest of the robot. The visual servo controller was implemented as a ROS action interface. This allows to divide the servoing task in a client and a server. The server runs the controller itself, computing the visual features and the correspondent camera velocities, while the client is used to set a new task goal (i.e. an AprilTag maker ID and desired pose)  and waits for the server to finish. The system to detect several markers at the same time and use the one provided by the user. When the marker goes out of the field of view of the camera, a simple algorithm tries to go back towards the lost target.

Finally, the performance of the controller was tested, achieving the desired behavior and achieving convergence to the desired pose in ten to twenty seconds.

\section{Future Work}

The present work is part of the Flypulator project at the Institute of Automation Engineering of the Technische Universit√§t Dresden. This aerial robot is going to be a fully-actuated hexarotor equipped with a multi-degree of freedom serial actuator, which will be used for aerial manipulation tasks.

The controller developed allows only the control of the translational and yaw degrees of freedom, so the next natural step would be to take into account the serial manipulator in the visual servoing task.

Due to this nature, taking into account the dynamic behavior of the aerial vehicle during strong maneuvers may not be important. Since aerial manipulations does not usually require them. However, due to the presence of the serial actuator arm, it would be recommended to consider its dynamics during the visual servoing . In this way the arm position can be controlled to minimize disturbances.

Partitioned control should be implemented to apply visual servoing for the control of the mobile platform and the arm. So depending on the distance to the target the camera velocities are applied using the mobile platform of moving the serial manipulator.

The current standard in visual servoing for aerial manipulators is a 4DOF plaform with a 6DOF arm. The weighted Jacobian matrix approach is combined with a priority task scheduler to take advantage of the over-actuation of the system and choose the degrees of freedom taking into account secondary task such as collision avoidance and joint limit reaching prevention.

Maintaining the camera on-board (eye-on-board) pointing to the end-effector would have the advantage of using the same camera also for visual flow. Otherwise, the camera could be placed in the end-effector to achieve a pure eye-in-hand configuration.

In order to achieve the control of six degrees of freedom, new visual features should be applied. Virtual random 3D point clouds with respect to an AprilTag of similar AR markers is the current strategy in all the aerial manipulators.

Using model based tracking could be used instead of markers, so the only think necessary to carry the visual servoing task would be the CAD model of the object to be manipulated. Allowing an easier interaction of the robot with the environment.